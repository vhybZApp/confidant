
services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    restart: unless-stopped
    ports:
      - "3000:8080"
    env_file:
      - .env
    volumes:
      - openwebui-data:/app/backend/data
      - olamma-data:/root/.olamma 
    depends_on:
      - llmlite

  llmlite:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    ports:
      - "4000:4000"
    env_file:
      - .env
    volumes:
      - ./llmlite-config.yml:/app/config.yml
    command: ["--config", "/app/config.yml"]

  omni-parser:
    image: ghcr.io/teleportagents/omni-parser:latest
    runtime: nvidia
    environment:
    - NVIDIA_VISIBLE_DEVICES=all
    - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # build: .
    restart: unless-stopped
    entrypoint: ""
    ports:
      - "8000:8000"
    volumes:
      - type: bind
        source: ./data/omni-weights
        target: /app/weights
    command: ["conda", "run", "-n", "omni", "/bin/bash", "-c", "./run.sh run", "--no-capture-output"]


volumes:
  openwebui-data:
    # external: true
    name: ${OPENWEBUI_VOLUME}
  olamma-data:
    # external: true
    name: ${OLAMMA_VOLUME}
    
  llmlite-data:
